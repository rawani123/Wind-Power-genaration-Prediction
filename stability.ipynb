{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('D:\\\\APSHAH\\\\backend\\\\part2final1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43730 entries, 0 to 43729\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   date          43730 non-null  object \n",
      " 1   p1            43730 non-null  float64\n",
      " 2   p2            43730 non-null  float64\n",
      " 3   p3            43730 non-null  float64\n",
      " 4   c1            43730 non-null  float64\n",
      " 5   c2            43730 non-null  float64\n",
      " 6   c3            43730 non-null  float64\n",
      " 7   stability     43730 non-null  int64  \n",
      " 8   g1            43730 non-null  float64\n",
      " 9   g2            43730 non-null  float64\n",
      " 10  g3            43730 non-null  float64\n",
      " 11  year          43730 non-null  int64  \n",
      " 12  month         43730 non-null  int64  \n",
      " 13  day           43730 non-null  int64  \n",
      " 14  hour          43730 non-null  int64  \n",
      " 15  minute        43730 non-null  int64  \n",
      " 16  seconds       43730 non-null  int64  \n",
      " 17  milliseconds  43730 non-null  int64  \n",
      "dtypes: float64(9), int64(8), object(1)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43730 entries, 0 to 43729\n",
      "Data columns (total 18 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          43730 non-null  datetime64[ns]\n",
      " 1   p1            43730 non-null  float64       \n",
      " 2   p2            43730 non-null  float64       \n",
      " 3   p3            43730 non-null  float64       \n",
      " 4   c1            43730 non-null  float64       \n",
      " 5   c2            43730 non-null  float64       \n",
      " 6   c3            43730 non-null  float64       \n",
      " 7   stability     43730 non-null  int64         \n",
      " 8   g1            43730 non-null  float64       \n",
      " 9   g2            43730 non-null  float64       \n",
      " 10  g3            43730 non-null  float64       \n",
      " 11  year          43730 non-null  int64         \n",
      " 12  month         43730 non-null  int64         \n",
      " 13  day           43730 non-null  int64         \n",
      " 14  hour          43730 non-null  int64         \n",
      " 15  minute        43730 non-null  int64         \n",
      " 16  seconds       43730 non-null  int64         \n",
      " 17  milliseconds  43730 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(9), int64(8)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming you have a DataFrame 'df' with columns as described\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = df[['p1', 'p2', 'p3', 'c1', 'c2', 'c3', 'g1', 'g2', 'g3', 'year', 'month', 'day', 'hour']]\n",
    "y = df['stability']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape the data to fit LSTM input shape (samples, time steps, features)\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_reshaped.shape[1], X_reshaped.shape[2])))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Assuming binary classification\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming you have a DataFrame 'df' with a datetime index and the target variable to forecast\n",
    "\n",
    "# Assuming your DataFrame is properly formatted and there are no missing values\n",
    "# If needed, check data types and missing values:\n",
    "# print(df.dtypes)  # Check data types of DataFrame columns\n",
    "# print(df.isnull().sum())  # Check for missing values in DataFrame\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(len(df) * 0.8)  # 80% for training\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "# Assuming your order tuple is correctly specified\n",
    "order = (5, 1, 0)  # Example order, you may need to tune this\n",
    "\n",
    "# Fit ARIMA model\n",
    "try:\n",
    "    model = ARIMA(train, order=order)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Forecast future values\n",
    "    forecast = model_fit.forecast(steps=len(test))[0]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(test, forecast)\n",
    "    mae = mean_absolute_error(test, forecast)\n",
    "    rmse = sqrt(mse)\n",
    "\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date            datetime64[ns]\n",
      "p1                     float64\n",
      "p2                     float64\n",
      "p3                     float64\n",
      "c1                     float64\n",
      "c2                     float64\n",
      "c3                     float64\n",
      "stability                int64\n",
      "g1                     float64\n",
      "g2                     float64\n",
      "g3                     float64\n",
      "year                     int64\n",
      "month                    int64\n",
      "day                      int64\n",
      "hour                     int64\n",
      "minute                   int64\n",
      "seconds                  int64\n",
      "milliseconds             int64\n",
      "dtype: object\n",
      "date            0\n",
      "p1              0\n",
      "p2              0\n",
      "p3              0\n",
      "c1              0\n",
      "c2              0\n",
      "c3              0\n",
      "stability       0\n",
      "g1              0\n",
      "g2              0\n",
      "g3              0\n",
      "year            0\n",
      "month           0\n",
      "day             0\n",
      "hour            0\n",
      "minute          0\n",
      "seconds         0\n",
      "milliseconds    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types of DataFrame columns\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values in DataFrame\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2184 entries, 0 to 2183\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       2184 non-null   datetime64[ns]\n",
      " 1   c1         2184 non-null   float64       \n",
      " 2   c2         2184 non-null   float64       \n",
      " 3   c3         2184 non-null   float64       \n",
      " 4   p1         2184 non-null   float64       \n",
      " 5   p2         2184 non-null   float64       \n",
      " 6   p3         2184 non-null   float64       \n",
      " 7   stability  2184 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(6), object(1)\n",
      "memory usage: 136.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the future dataset\n",
    "future_df = pd.read_csv(\"D:\\\\APSHAH\\\\backend\\\\grid_stability_3months_validation_data.csv\")\n",
    "\n",
    "# Preprocess the future dataset\n",
    "\n",
    "\n",
    "future_df['date'] = pd.to_datetime(future_df['date'])\n",
    "\n",
    "future_df.info()\n",
    "\n",
    "# Evaluate the predictions (if you have actual labels)\n",
    "# accuracy = accuracy_score(actual_labels, future_predictions)\n",
    "# print(\"Accuracy on future data:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANIRUDDHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.2271\n",
      "Epoch 2/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1993\n",
      "Epoch 3/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1979\n",
      "Epoch 4/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1979\n",
      "Epoch 5/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1962\n",
      "Epoch 6/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1959\n",
      "Epoch 7/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1956\n",
      "Epoch 8/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1952\n",
      "Epoch 9/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1936\n",
      "Epoch 10/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1960\n",
      "Epoch 11/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1958\n",
      "Epoch 12/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1974\n",
      "Epoch 13/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1956\n",
      "Epoch 14/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1975\n",
      "Epoch 15/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1958\n",
      "Epoch 16/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1948\n",
      "Epoch 17/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1939\n",
      "Epoch 18/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1951\n",
      "Epoch 19/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1956\n",
      "Epoch 20/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1949\n",
      "Epoch 21/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1951\n",
      "Epoch 22/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1960\n",
      "Epoch 23/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1953\n",
      "Epoch 24/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1948\n",
      "Epoch 25/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1940\n",
      "Epoch 26/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1962\n",
      "Epoch 27/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1957\n",
      "Epoch 28/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1948\n",
      "Epoch 29/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1970\n",
      "Epoch 30/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1953\n",
      "Epoch 31/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1958\n",
      "Epoch 32/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1958\n",
      "Epoch 33/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1968\n",
      "Epoch 34/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1951\n",
      "Epoch 35/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1953\n",
      "Epoch 36/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1957\n",
      "Epoch 37/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1956\n",
      "Epoch 38/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1950\n",
      "Epoch 39/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1946\n",
      "Epoch 40/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1944\n",
      "Epoch 41/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1950\n",
      "Epoch 42/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1929\n",
      "Epoch 43/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1961\n",
      "Epoch 44/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1953\n",
      "Epoch 45/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1945\n",
      "Epoch 46/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1953\n",
      "Epoch 47/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1948\n",
      "Epoch 48/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1937\n",
      "Epoch 49/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.1950\n",
      "Epoch 50/50\n",
      "\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1939\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "      Stability predicted              date\n",
      "0                0.851352  01-01-2024 00:00\n",
      "1                0.382074  01-01-2024 01:00\n",
      "2                0.691130  01-01-2024 02:00\n",
      "3                0.348396  01-01-2024 03:00\n",
      "4                0.477950  01-01-2024 04:00\n",
      "...                   ...               ...\n",
      "2179             0.629715  31-03-2024 19:00\n",
      "2180             0.448647  31-03-2024 20:00\n",
      "2181             0.255634  31-03-2024 21:00\n",
      "2182             0.263959  31-03-2024 22:00\n",
      "2183             0.244372  31-03-2024 23:00\n",
      "\n",
      "[2184 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming you have a DataFrame 'df' with columns including 'p1', 'p2', 'p3', 'c1', 'c2', 'c3', 'g1', 'g2', 'g3', 'year', 'month', 'day', 'hour', and 'stability'\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = df[['p1', 'p2', 'p3', 'c1', 'c2', 'c3']]\n",
    "y = df['stability']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape the data to fit LSTM input shape (samples, time steps, features)\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_reshaped.shape[1], X_reshaped.shape[2])))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_reshaped, y, epochs=50, batch_size=32)\n",
    "\n",
    "# Read future data from Excel file\n",
    "future_data = pd.read_csv('D:\\\\APSHAH\\\\backend\\\\grid_stability_3months_validation_data.csv')\n",
    "\n",
    "# Prepare features for predicting the next three months\n",
    "future_features = future_data[['p1', 'p2', 'p3', 'c1', 'c2', 'c3']]\n",
    "\n",
    "# Check if the number of features in the future data matches the training data\n",
    "if len(future_features.columns) != X.shape[1]:\n",
    "    print(\"Number of features in future data does not match the training data.\")\n",
    "else:\n",
    "    # Normalize the future features\n",
    "    future_features_scaled = scaler.transform(future_features)\n",
    "    \n",
    "    # Reshape the future features data\n",
    "    future_features_reshaped = future_features_scaled.reshape((future_features_scaled.shape[0], 1, future_features_scaled.shape[1]))\n",
    "    \n",
    "    # Make predictions for the next three months\n",
    "    future_predictions = model.predict(future_features_reshaped)\n",
    "    \n",
    "    # Create a new DataFrame to store predictions\n",
    "    predictions_df = pd.DataFrame({'Stability predicted': future_predictions.flatten()})\n",
    "    \n",
    "    # Add DateTime column to the predictions DataFrame\n",
    "    predictions_df['date'] = future_data['date']\n",
    "    \n",
    "    # Print the future predictions DataFrame\n",
    "    print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Stability predicted              date\n",
       "0                0.851352  01-01-2024 00:00\n",
       "1                0.382074  01-01-2024 01:00\n",
       "2                0.691130  01-01-2024 02:00\n",
       "3                0.348396  01-01-2024 03:00\n",
       "4                0.477950  01-01-2024 04:00\n",
       "...                   ...               ...\n",
       "2179             0.629715  31-03-2024 19:00\n",
       "2180             0.448647  31-03-2024 20:00\n",
       "2181             0.255634  31-03-2024 21:00\n",
       "2182             0.263959  31-03-2024 22:00\n",
       "2183             0.244372  31-03-2024 23:00\n",
       "\n",
       "[2184 rows x 2 columns]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Stability predicted              date\n",
      "0                       1  01-01-2024 00:00\n",
      "1                       0  01-01-2024 01:00\n",
      "2                       1  01-01-2024 02:00\n",
      "3                       0  01-01-2024 03:00\n",
      "4                       0  01-01-2024 04:00\n",
      "...                   ...               ...\n",
      "2179                    1  31-03-2024 19:00\n",
      "2180                    0  31-03-2024 20:00\n",
      "2181                    0  31-03-2024 21:00\n",
      "2182                    0  31-03-2024 22:00\n",
      "2183                    0  31-03-2024 23:00\n",
      "\n",
      "[2184 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "predictions_df['Stability predicted'] = predictions_df['Stability predicted'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Stability predicted              date\n",
      "0                 Stable  01-01-2024 00:00\n",
      "1               Unstable  01-01-2024 01:00\n",
      "2                 Stable  01-01-2024 02:00\n",
      "3               Unstable  01-01-2024 03:00\n",
      "4               Unstable  01-01-2024 04:00\n",
      "...                  ...               ...\n",
      "2179              Stable  31-03-2024 19:00\n",
      "2180            Unstable  31-03-2024 20:00\n",
      "2181            Unstable  31-03-2024 21:00\n",
      "2182            Unstable  31-03-2024 22:00\n",
      "2183            Unstable  31-03-2024 23:00\n",
      "\n",
      "[2184 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Map binary values to labels\n",
    "predictions_df['Stability predicted'] = predictions_df['Stability predicted'].map({0: 'Unstable', 1: 'Stable'})\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data=future_data.merge(predictions_df, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Stable predictions: 22.57%\n",
      "Percentage of Unstable predictions: 77.43%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the count of stable and unstable predictions\n",
    "stable_count = future_data['Stability predicted'].eq('Stable').sum()\n",
    "unstable_count = future_data['Stability predicted'].eq('Unstable').sum()\n",
    "\n",
    "# Calculate the total number of predictions\n",
    "total_predictions = len(future_data)\n",
    "\n",
    "# Calculate the percentage of stable and unstable predictions\n",
    "stable_percentage = (stable_count / total_predictions) * 100\n",
    "unstable_percentage = (unstable_count / total_predictions) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Percentage of Stable predictions: {stable_percentage:.2f}%\")\n",
    "print(f\"Percentage of Unstable predictions: {unstable_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                   date        c1        c2        c3        p1        p2  \\\n",
       "0     01-01-2024 00:00 -1.940140 -0.798906 -1.171796  0.209084  0.125430   \n",
       "1     01-01-2024 01:00 -1.431471 -0.523455 -1.481922  0.659985  0.565942   \n",
       "2     01-01-2024 02:00 -0.791358 -1.134176 -1.023553  0.083137  0.411446   \n",
       "3     01-01-2024 03:00 -1.291775 -1.943314 -1.536861  0.805780  0.431950   \n",
       "4     01-01-2024 04:00 -0.532246 -1.734586 -0.845011  0.067430  0.766504   \n",
       "...                ...       ...       ...       ...       ...       ...   \n",
       "2179  31-03-2024 19:00 -1.593110 -1.096450 -1.287426  0.549650  0.107777   \n",
       "2180  31-03-2024 20:00 -1.987940 -1.172603 -1.136143  0.202879  0.613149   \n",
       "2181  31-03-2024 21:00 -0.974514 -1.665259 -1.477446  0.772918  0.565030   \n",
       "2182  31-03-2024 22:00 -0.787608 -1.179217 -0.716963  0.579977  0.898658   \n",
       "2183  31-03-2024 23:00 -1.399564 -0.925921 -1.463104  0.999409  0.526355   \n",
       "\n",
       "            p3 stability  \n",
       "0     0.056619    stable  \n",
       "1     0.252764    stable  \n",
       "2     0.376176    stable  \n",
       "3     0.222319  unstable  \n",
       "4     0.349372    stable  \n",
       "...        ...       ...  \n",
       "2179  0.327430    stable  \n",
       "2180  0.487372  unstable  \n",
       "2181  0.422756    stable  \n",
       "2182  0.214563  unstable  \n",
       "2183  0.274401  unstable  \n",
       "\n",
       "[2184 rows x 8 columns]>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Stable predictions: 34.94%\n",
      "Percentage of Unstable predictions: 65.06%\n"
     ]
    }
   ],
   "source": [
    "stable_count = future_data['stability'].eq('stable').sum()\n",
    "unstable_count = future_data['stability'].eq('unstable').sum()\n",
    "\n",
    "# Calculate the total number of predictions\n",
    "total_predictions = len(future_data)\n",
    "\n",
    "# Calculate the percentage of stable and unstable predictions\n",
    "stable_percentage = (stable_count / total_predictions) * 100\n",
    "unstable_percentage = (unstable_count / total_predictions) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Percentage of Stable predictions: {stable_percentage:.2f}%\")\n",
    "print(f\"Percentage of Unstable predictions: {unstable_percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
